{"pages":[],"posts":[{"title":"用爬虫分析了2020年一年的热搜，我发现了什么？","text":"前言2020年是艰难的一年，但即使再难，也都过去了。 分析一下2020年的新闻热搜，可以大致了解网民都在关注什么。 微博热搜以娱乐为主，头条的热搜更偏向民生与时事。今天，我们先分析分析微博一整年的热搜。 数据抓取由于微博平台不能查看历史热搜，本文所有的数据都是从云合数据旗下的一个热搜榜抓取的，具体地址见文末。 抓取数据的代码是用Python写的，代码非常简单，就是模拟请求和数据统计两个部分。 抓取到了从2020年1月1日到2020年12月31日一共35901条数据，将抓取的数据存入Excel，数据格式如下图： 数据分析用pyecharts做可视化分析，先将所有的热搜数据按照热度做成一个词云。 “词云”是对海量文字内容中出现频率较高的视觉突出，即出现越多的“关键词”字体越大。这里用热度替代了出现次数 结合微博热搜话题搜索量TOP10，哪些热搜热度高就一目了然了。 罗志祥的分手引爆微博，两条热搜霸占全年热搜榜前二名，搜索指数加起来超过了5700万。要知道，鹿晗官宣和关晓彤在一起那一条热搜也只有1700多万搜索指数。 美国总统特朗普从看不起新冠，到最后夫妻双双确诊，打脸来的不要太快。“特朗普夫妇确诊新冠”的新闻也曾引爆微博甚至全球。 在前十榜单里面，“李文亮医生去世”和“李文亮仍在抢救”这两条热搜引人注目。 李文亮医生的事迹在当时引起了一阵轰动，一年的时间快要过去了，“李文亮”这个名字也慢慢不被人们提起，后人也许也不会知道这段故事。但人们不提，不代表忘记，互联网也是有记忆的，每当回忆起这次疫情，李文亮医生依旧站在那里。让我们向英雄致敬！ 热词回顾一年的热搜，和疫情相关的词汇出现的比较高频，“抗疫”成为了2020年中国甚至世界的主旋律。 2020年中美关系因美方一意孤行而比较紧张，加上美国疫情的不可控发展，“美国”和“特朗普”也频频登上热搜。 我还生成了“道歉”，“帅”，“结婚”，“分手”，“心疼”等关键字词云，大部分都是和娱乐圈相关的，今天你道歉，明天我发声。我的感觉就是，贵圈真乱。 人名出现次数排行榜统计了上热搜次数最多的前40个人。大部分都是娱乐圈的，其中周杰伦是榜中唯一一个没有开通微博的明星。 美国现总统特朗普和下任总统拜登因为选举和美国疫情不可控等因素频频上热搜，特朗普更是差不多平均一天上一次热搜。 获颁“共和国勋章”的敢医敢言的钟南山2020年上了196次热搜，从非典到新冠，攸关生命的大考，他从不缺位，向钟老致敬。 通过《陈情令》爆火的王一博和肖战，分别排在第四名和第六名。 有意思是，从2011年《宫》播出之后，杨幂每年上热搜次数就没有跌出过前10。 2020年，是直播带货蓬勃发展的一年，李佳琦和薇娅两个带货之王也频频登上热搜，甚至李佳琦以103次的成绩排到了第八名。 归国四子黄子韬、鹿晗、吴亦凡和张艺兴也依然保持热度，都有着比较靠前的排名。 2020年湖人队民宿科比坠机身亡，因此也上了69次热搜，R.I.P。 不开通微博的周董周杰伦，凭借自己的实力和强大的粉丝群体，强行上了59次热搜，是为数不多不靠炒作就能登上这个榜单的明星了。 最近一个月才大火的丁真，就以47次热搜的成绩登上排行榜，同时丁真也是12月上热搜次数最多的人。 总结 2020年微博热搜主旋律——抗疫 2020年上微博热搜次数最多的人——特朗普 2020年上微博热搜次数最多的娱乐明星——易烊千玺 2020年热度最高的微博热搜——罗志祥周扬青分手 以上所有的数据都基于热搜平台，一切信息都可以在平台查询。 写到这里，如果你问“买热搜”真的存在吗？ 我会告诉你，“买热搜”真的存在。我分析了几年的微博热搜数据，确实发现了一些问题，有些热搜非常莫名其妙，热度来得快去的也快，再结合一下其他平台的数据就能容易分析出这个热搜的真实性了。 但如果你问我有哪些明星买了热搜，我不能告诉你，为什么呢？因为我告诉你我的号就没了。 如果你实在有兴趣，公众号后台回复“热搜”，我会给你一份2020一整年的微博热搜数据Excel，你可以自己去慢慢分析。 写在最后以上所有的数据都基于热搜平台，一切信息都可以在平台查询。 本人才疏学浅，如有错误欢迎指出！ 最后，感谢您的阅读。 您的点赞和转发都是对我最大的支持，十分欢迎并感谢您的关注。 本文所有的资料和源码关注公众号‘CoderW’回复‘热搜’即可获取 热搜平台：https://www.enlightent.cn/research/rank/weiboSearchRank 参考文章: https://mp.weixin.qq.com/s/BD1rqPCQvhYP4i4bef_BAg。","link":"/resou/"},{"title":"来自未来的技术——ZGC垃圾回收期","text":"ZGC介绍ZGC（The Z Garbage Collector）是JDK 11中推出的一款追求极致低延迟的实验性质的垃圾收集器，它曾经设计目标包括： 停顿时间不超过10ms； 停顿时间不会随着堆的大小，或者活跃对象的大小而增加； 支持8MB~4TB级别的堆（未来支持16TB）。 当初，提出这个目标的时候，有很多人都觉得设计者在吹牛逼。 但今天看来，这些“吹下的牛逼”都在一个个被实现。 基于最新的JDK15来看，“停顿时间不超过10ms”和“支持16TB的堆”这两个目标已经实现，并且官方明确指出JDK15中的ZGC不再是实验性质的垃圾收集器，且建议投入生产了。 本文会从ZGC的设计思路出发，讲清楚为何ZGC能在低延时场景中的应用中有着如此卓越的表现。 核心技术多重映射为了能更好的理解ZGC的内存管理，我们先看一下这个例子： 你在你爸爸妈妈眼中是儿子，在你女朋友眼中是男朋友。在全世界人面前就是最帅的人。你还有一个名字，但名字也只是你的一个代号，并不是你本人。将这个关系画一张映射图表示： 在你爸爸的眼中，你就是儿子； 在你女朋友的眼中，你就说男朋友； 站在全世界角度来看，你就说世界上最帅的人； 假如你的名字是全世界唯一的，通过“你的名字”、“你爸爸的儿子”、“你女朋友的男朋友”，“世界上最帅的人”最后定位到的都是你本人。 现在我们再来看看ZGC的内存管理。 ZGC为了能高效、灵活地管理内存，实现了两级内存管理：虚拟内存和物理内存，并且实现了物理内存和虚拟内存的映射关系。这和操作系统中虚拟地址和物理地址设计思路基本一致。 当应用程序创建对象时，首先在堆空间申请一个虚拟地址，ZGC同时会为该对象在Marked0、Marked1和Remapped三个视图空间分别申请一个虚拟地址，且这三个虚拟地址对应同一个物理地址。 图中的Marked0、Marked1和Remapped三个视图是什么意思呢？ 对照上面的例子，这三个视图分别对应的就是”你爸爸眼中”，“你女朋友的眼中”，“全世界人眼中”。 而三个视图里面的地址，都是虚拟地址，对应的是“你爸爸眼中的儿子”，“你女朋友眼中的男朋友”…… 最后，这些虚地址都能定位到一个物理地址，这个物理地址对应上面例子中的“你本人”。 用一段简单的Java代码表示就是这样的： 在ZGC中这三个空间在同一时间点有且仅有一个空间有效。 为什么这么设计呢？这就是ZGC的高明之处，利用虚拟空间换时间，这三个空间的切换是由垃圾回收的不同阶段触发的，通过限定三个空间在同一时间点有且仅有一个空间有效高效的完成GC过程的并发操作，具体实现会后面讲ZGC并发处理算法的部分再详细描述。 染色指针在讲ZGC并发处理算法之前，还需要补充一个知识点——染色指针。 我们都知道，之前的垃圾收集器都是把GC信息（标记信息、GC分代年龄..）存在对象头的Mark Word里。举个例子： 如果某个人是个垃圾人，就在这个人的头上盖一个“垃圾”的章；如果这个人不是垃圾了，就把这个人头上的“垃圾”印章洗掉。 而ZGC是这样做的： 如果某个人是垃圾人。就在这个人的身份证信息里面标注这个人是个垃圾，以后不管这个人在哪刷身份证，别人都知道他是个垃圾人了。也许哪一天，这个人醒悟了不再是垃圾人了，就把这个人身份证里面的“垃圾”标志去掉。 在这例子中，“这个人”就是一个对象，而“身份证”就是指向这个对象的指针。 ZGC将信息存储在指针中，这种技术有一个高大上的名字——染色指针（Colored Pointer）。 在64位的机器中，对象指针是64位的。 ZGC使用64位地址空间的第0~43位存储对象地址，2^44 = 16TB，所以ZGC最大支持16TB的堆。 而第44~47位作为颜色标志位，Marked0、Marked1和Remapped代表三个视图标志位，Finalizable表示这个对象只能通过finalizer才能访问。 第48~63位固定为0没有利用。 读屏障读屏障是JVM向应用代码插入一小段代码的技术。当应用线程从堆中读取对象引用时，就会执行这段代码。千万不要把这个读屏障和Java内存模型里面的读屏障搞混了，两者根本不是同一个东西，ZGC中的读屏障更像是一种AOP技术，在字节码层面或者编译代码层面给读操作增加一个额外的处理。 读屏障实例： 123456Object o = obj.FieldA // 从堆中读取对象引用，需要加入读屏障&lt;load barrier needed here&gt; Object p = o // 无需加入读屏障，因为不是从堆中读取引用o.dosomething() // 无需加入读屏障，因为不是从堆中读取引用int i = obj.FieldB // 无需加入读屏障，因为不是对象引用 ZGC中读屏障的代码作用： GC线程和应用线程是并发执行的，所以存在应用线程去A对象内部的引用所指向的对象B的时候，这个对象B正在被GC线程移动或者其他操作，加上读屏障之后，应用线程会去探测对象B是否被GC线程操作，然后等待操作完成再读取对象，确保数据的准确性。具体的探测和操作步骤如下： 这样会影响程序的性能吗？ 会。据测试，最多百分之4的性能损耗。但这是ZGC并发转移的基础，为了降低STW，设计者认为这点牺牲是可接受的。 ZGC并发处理算法ZGC并发处理算法利用全局空间视图的切换和对象地址视图的切换，结合SATB算法实现了高效的并发。 以上所有的铺垫，都是为了讲清楚ZGC的并发处理算法，在一些博文上，都说染色指针和读屏障是ZGC的核心，但都没有讲清楚两者是如何在算法里面被利用的，我认为，ZGC的并发处理算法才是ZGC的核心，染色指针和读屏障只不过是为算法服务而已。 ZGC的并发处理算法三个阶段的全局视图切换如下： 初始化阶段：ZGC初始化之后，整个内存空间的地址视图被设置为Remapped 标记阶段：当进入标记阶段时的视图转变为Marked0（以下皆简称M0）或者Marked1（以下皆简称M1） 转移阶段：从标记阶段结束进入转移阶段时的视图再次设置为Remapped 标记阶段标记阶段全局视图切换到M0视图。因为应用程序和标记线程并发执行，那么对象的访问可能来自标记线程和应用程序线程。 在标记阶段结束之后，对象的地址视图要么是M0，要么是Remapped。 如果对象的地址视图是M0，说明对象是活跃的； 如果对象的地址视图是Remapped，说明对象是不活跃的，即对象所使用的内存可以被回收。 当标记阶段结束后，ZGC会把所有活跃对象的地址存到对象活跃信息表，活跃对象的地址视图都是M0。 转移阶段转移阶段切换到Remapped视图。因为应用程序和转移线程也是并发执行，那么对象的访问可能来自转移线程和应用程序线程。 至此，ZGC的一个垃圾回收周期中，并发标记和并发转移就结束了。 为何要设计M0和M1我们提到在标记阶段存在两个地址视图M0和M1，上面的算法过程显示只用到了一个地址视图，为什么设计成两个？简单地说是为了区别前一次标记和当前标记。 ZGC是按照页面进行部分内存垃圾回收的，也就是说当对象所在的页面需要回收时，页面里面的对象需要被转移，如果页面不需要转移，页面里面的对象也就不需要转移。 如图，这个对象在第二次GC周期开始的时候，地址视图还是M0。如果第二次GC的标记阶段还切到M0视图的话，就不能区分出对象是活跃的，还是上一次垃圾回收标记过的。这个时候，第二次GC周期的标记阶段切到M1视图的话就可以区分了，此时这3个地址视图代表的含义是： M1：本次垃圾回收中识别的活跃对象。 M0：前一次垃圾回收的标记阶段被标记过的活跃对象，对象在转移阶段未被转移，但是在本次垃圾回收中被识别为不活跃对象。 Remapped：前一次垃圾回收的转移阶段发生转移的对象或者是被应用程序线程访问的对象，但是在本次垃圾回收中被识别为不活跃对象。 现在，我们可以回答“使用地址视图和染色指针有什么好处”这个问题了 使用地址视图和染色指针可以加快标记和转移的速度。以前的垃圾回收器通过修改对象头的标记位来标记GC信息，这是有内存存取访问的，而ZGC通过地址视图和染色指针技术，无需任何对象访问，只需要设置地址中对应的标志位即可。这就是ZGC在标记和转移阶段速度更快的原因。 当GC信息不再存储在对象头上时而存在引用指针上时，当确定一个对象已经无用的时候，可以立即重用对应的内存空间，这是把GC信息放到对象头所做不到的。 ZGC步骤ZGC采用的是标记-复制算法，标记、转移和重定位阶段几乎都是并发的，ZGC垃圾回收周期如下图所示： ZGC只有三个STW阶段：初始标记，再标记，初始转移。 其中，初始标记和初始转移分别都只需要扫描所有GC Roots，其处理时间和GC Roots的数量成正比，一般情况耗时非常短； 再标记阶段STW时间很短，最多1ms，超过1ms则再次进入并发标记阶段。即，ZGC几乎所有暂停都只依赖于GC Roots集合大小，停顿时间不会随着堆的大小或者活跃对象的大小而增加。与ZGC对比，G1的转移阶段完全STW的，且停顿时间随存活对象的大小增加而增加。 ZGC的发展ZGC诞生于JDK11，经过不断的完善，JDK15中的ZGC已经不再是实验性质的了。 从只支持Linux/x64，到现在支持多平台；从不支持指针压缩，到支持压缩类指针….. 在JDK16，ZGC将支持并发线程栈扫描（Concurrent Thread Stack Scanning），根据SPECjbb2015测试结果，实现并发线程栈扫描之后，ZGC的STW时间又能降低一个数量级，停顿时间将进入毫秒时代。 ZGC已然是一款优秀的垃圾收集器了，它借鉴了Pauseless GC，也似乎在朝着C4 GC的方向发展——引入分代思想。 Oracle的努力，让我们开发者看到了商用级别的GC“飞入寻常百姓家”的希望，随着JDK的发展，我相信在未来的某一天，JVM调优这种反人类的操作将不复存在，底层的GC会自适应各种情况自动优化。 ZGC确实是Java的最前沿的技术，但在G1都没有普及的今天，谈论ZGC似乎为时过早。但也许我们探讨的不是ZGC，而是ZGC背后的设计思路。 希望你能有所收获！ 写在最后为了对每一篇发出去的文章负责，力求准确，我一般是参考官方文档和业界权威的书籍，有些时候，还需要看一些论文，看一部分源代码。而官方文档和论文一般都是英文，对于一个英语四级只考了456分的人来说，非常艰难，整个过程都是谷歌翻译和有道词典陪伴着我的。因为一些专业术语翻译的不够准确，还需要英文和翻译对照慢慢理解。 但即使这样，也难免会有纰漏，如果你发现了，欢迎提出，我会对其修正。 你的正反馈对我来说非常重要，点个赞，点个关注都是对我最大的支持！ 如果你有什么想和我交流的，可以关注我的微信公众号“CoderW”，非常欢迎并感谢您的关注！ 谢谢您的阅读，我们下期再见！ 参考资料 https://mp.weixin.qq.com/s/ag5u2EPObx7bZr7hkcrOTg 《新一代垃圾回收器ZGC设计与实现》","link":"/zgc/"},{"title":"深入汇编指令解析Java关键字volatile","text":"volatile是什么volatile关键字是Java提供的一种轻量级同步机制。它能够保证可见性和有序性，但是不能保证原子性 可见性对于volatile的可见性，先看看这段代码的执行 flag默认为true 创建一个线程A去判断flag是否为true，如果为true循环执行i++操作 两秒后，创建另一个线程B将flag修改为false 线程A没有感知到flag已经被修改成false了，不能跳出循环 这相当于啥呢？相当于你的女神和你说，你好好努力，年薪百万了就嫁给你，你听了之后，努力赚钱。3年之后，你年薪百万了，回去找你女神，结果发现你女神结婚了，她结婚的消息根本没有告诉你！难不难受？ 女神结婚可以不告诉你，可是Java代码中的属性都是存在内存中，一个线程的修改为什么另一个线程为什么不可见呢？这就不得不提到Java中的内存模型了，Java中的内存模型，简称JMM，JMM定义了线程和主内存之间的抽象关系，定义了线程之间的共享变量存储在主内存中，每个线程都有一个私有的本地内存，本地内存中存储了该线程以读/写共享变量的副本，它涵盖了缓存、写缓冲区、寄存器以及其他的硬件和编译器优化。 注意！JMM是一个屏蔽了不同操作系统架构的差异的抽象概念，只是一组Java规范。 了解了JMM，现在我们再回顾一下文章开头的那段代码，为什么线程B修改了flag线程A看到的还是原来的值呢？ 因为线程A复制了一份刚开始的flage=true到本地内存，之后线程A使用的flag都是这个复制到本地内存的flag。 线程B修改了flag之后，将flag的值刷新到主内存，此时主内存的flag值变成了false。 线程A是不知道线程B修改了flag，一直用的是本地内存的flag = true。 那么，如何才能让线程A知道flag被修改了呢？或者说怎么让线程A本地内存中缓存的flag无效，实现线程间可见呢？用volatile修饰flag就可以做到: 我们可以看到，用volatile修饰flag之后，线程B修改flag之后线程A是能感知到的，说明了volatile保证了线程同步之间的可见性。 重排序在阐述volatile有序性之前，需要先补充一些关于重排序的知识。 重排序是指编译器和处理器为了优化程序性能而对指令序列进行重新排序的一种手段。 为什么要有重排序呢？简单来说，就是为了提升执行效率。为什么能提升执行效率呢？我们看下面这个例子： 可以看到重排序之后CPU实际执行省略了一个读取和写回的操作，也就间接的提升了执行效率。 有一点必须强调的是，上图的例子只是为了让读者更好的理解为什么重排序能提升执行效率，实际上Java里面的重排序并不是基于代码级别的，从代码到CPU执行之间还有很多个阶段，CPU底层还有一些优化，实际上的执行流程可能并不是上图的说的那样。不必过于纠结于此。 重排序可以提高程序的运行效率，但是必须遵循as-if-serial语义。as-if-serial语义是什么呢？简单来说，就是不管你怎么重排序，你必须保证不管怎么重排序，单线程下程序的执行结果不能被改变。 有序性上面我们已经介绍了Java有重排序情况，现在我们再来聊一聊volatile的有序性。 先看一个经典的面试题：为什么DDL（double check lock）单例模式需要加volatile关键字？ 因为singleton = new Singleton()不是一个原子操作，大概要经过这几个步骤： 分配一块内存空间 调用构造器，初始化实例 singleton指向分配的内存空间 实际执行的时候，可能发生重排序，导致实际执行步骤是这样的： 申请一块内存空间 singleton指向分配的内存空间 调用构造器，初始化实例 在singleton指向分配的内存空间之后，singleton就不为空了。但是在没有调用构造器初始化实例之前，这个对象还处于半初始化状态，在这个状态下，实例的属性都还是默认属性，这个时候如果有另一个线程调用getSingleton()方法时，会拿到这个半初始化的对象，导致出错。 而加volatile修饰之后，就会禁止重排序，这样就能保证在对象初始化完了之后才把singleton指向分配的内存空间，杜绝了一些不可控错误的产生。volatile提供了happens-before保证，对volatile变量的写入happens-before所有其他线程后续对的读操作。 原理从上面的DDL单例用例来看，在并发情况下，重排序的存在会导致一些未知的错误。而加上volatile之后会防止重排序，那volatile是如何禁止重排序呢？ 为了实现volatile的内存语义，JMM会限制特定类型的编译器和处理器重排序，JMM会针对编译器制定volatile重排序规则表： 总结来说就是： 第二个操作是volatile写，不管第一个操作是什么都不会重排序 第一个操作是volatile读，不管第二个操作是什么都不会重排序 第一个操作是volatile写，第二个操作是volatile读，也不会发生重排序 如何保证这些操作不会发送重排序呢？就是通过插入内存屏障保证的，JMM层面的内存屏障分为读（load）屏障和写（Store）屏障，排列组合就有了四种屏障。对于volatile操作，JMM内存屏障插入策略： 在每个volatile写操作的前面插入一个StoreStore屏障 在每个volatile写操作的后面插入一个StoreLoad屏障 在每个volatile读操作的后面插入一个LoadLoad屏障 在每个volatile读操作的后面插入一个LoadStore屏障 上面的屏障都是JMM规范级别的，意思是，按照这个规范写JDK能保证volatile修饰的内存区域的操作不会发送重排序。 在硬件层面上，也提供了一系列的内存屏障来提供一致性的能力。拿X86平台来说，主要提供了这几种内存屏障指令： lfence指令：在lfence指令前的读操作当必须在lfence指令后的读操作前完成，类似于读屏障 sfence指令：在sfence指令前的写操作当必须在sfence指令后的写操作前完成，类似于写屏障 mfence指令： 在mfence指令前的读写操作当必须在mfence指令后的读写操作前完成，类似读写屏障。 JMM规范需要加这么多内存屏障，但实际情况并不需要加这么多内存屏障。以我们常见的X86处理器为例，X86处理器不会对读-读、读-写和写-写操作做重排序，会省略掉这3种操作类型对应的内存屏障，仅会对写-读操作做重排序。所以volatile写-读操作只需要在volatile写后插入StoreLoad屏障。在《The JSR-133 Cookbook for Compiler Writers》中，也很明确的指出了这一点： 而在x86处理器中，有三种方法可以实现实现StoreLoad屏障的效果，分别为： mfence指令：上文提到过，能实现全能型屏障，具备lfence和sfence的能力。 cpuid指令：cpuid操作码是一个面向x86架构的处理器补充指令，它的名称派生自CPU识别，作用是允许软件发现处理器的详细信息。 lock指令前缀：总线锁。lock前缀只能加在一些特殊的指令前面。 实际上HotSpot关于volatile的实现就是使用的lock指令，只在volatile标记的地方加上带lock前缀指令操作，并没有参照JMM规范的屏障设计而使用对应的mfence指令。 加上-XX:+UnlockDiagnosticVMOptions -XX:+PrintAssembly -XcompJVM参数再次执行main方法，在打印的汇编码中，我们也可以看到有一个lock addl $0x0,(%rsp)的操作。 在源码中也可以得到验证： lock addl $0x0,(%rsp)后面的addl $0x0,(%rsp)其实是一个空操作。add是加的意思，0x0是16进制的0，rsp是一种类型寄存器，合起来就是把寄存器的值加0，加0是不是等于什么都没有做？这段汇编码仅仅是lock指令的一个载体而已。其实上文也有提到过，lock前缀只能加在一些特殊的指令前面，add就是其中一个指令。 至于Hotspot为什么要使用lock指令而不是mfence指令，按照我的理解，其实就是省事，实现起来简单。因为lock功能过于强大，不需要有太多的考虑。而且lock指令优先锁缓存行，在性能上，lock指令也没有想象中的那么差，mfence指令更没有想象中的好。所以，使用lock是一个性价比非常高的一个选择。而且，lock也有对可见性的语义说明。 在《IA-32架构软件开发人员手册》的指令表中找到lock： 我不打算在这里深入阐述lock指令的实现原理和细节，这很容易陷入堆砌技术术语中，而且也超出了本文的范围，有兴趣的可以去看看《IA-32架构软件开发人员手册》。 我们只需要知道lock的这几个作用就可以了： 确保后续指令执行的原子性。在Pentium及之前的处理器中，带有lock前缀的指令在执行期间会锁住总线，使得其它处理器暂时无法通过总线访问内存，很显然，这个开销很大。在新的处理器中，Intel使用缓存锁定来保证指令执行的原子性，缓存锁定将大大降低lock前缀指令的执行开销。 禁止该指令与前面和后面的读写指令重排序。 把写缓冲区的所有数据刷新到内存中。 总结来说，就是lock指令既保证了可见性也保证了原子性。 重要的事情再说一遍，是lock指令既保证了可见性也保证了原子性，和什么缓冲一致性协议啊，MESI什么的没有一点关系。 为了不让你把缓存一致性协议和JMM混淆，在前面的文章中，我特意没有提到过缓存一致性协议，因为这两者本不是一个维度的东西，存在的意义也不一样，这一部分，我们下次再聊。 总结全文重点是围绕volatile的可见性和有序性展开的，其中花了不少的部分篇幅描述了一些计算机底层的概念，对于读者来说可能过于无趣，但如果你能认真看完，我相信你或多或少也会有一点收获。 不去深究，volatile只是一个普通的关键字。深入探讨，你会发现volatile是一个非常重要的知识点。volatile能将软件和硬件结合起来，想要彻底弄懂，需要深入到计算机的最底层。但如果你做到了。你对Java的认知一定会有进一步的提升。 只把眼光放在Java语言，似乎显得非常局限。发散到其他语言，C语言，C++里面也都有volatile关键字。我没有看过C语言，C++里面volatile关键字是如何实现的，但我相信底层的原理一定是相通的。 写在最后本着对每一篇发出去的文章负责的原则，文中涉及知识理论，我都会尽量在官方文档和权威书籍找到并加以验证。但即使这样，我也不能保证文中每个点都是正确的，如果你发现错误之处，欢迎指出，我会对其修正。 创作不易，你的正反馈对我来说非常重要！点个赞，点个再看，点个关注甚至评论区发送一条666都是对我最大的支持！ 我是CoderW，一个普通的程序员。 谢谢你的阅读，我们下期再见！ 个人公众号“CoderW”，欢迎并十分感谢你的关注 参考资料 JSR-133: http://gee.cs.oswego.edu/dl/jmm/cookbook.html 《Java并发编程的艺术》 《深入理解Java虚拟机》第三版 《IA-32+架构软件开发人员手册》","link":"/volatile/"}],"tags":[{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"数据分析","slug":"数据分析","link":"/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"JVM","slug":"JVM","link":"/tags/JVM/"},{"name":"并发编程","slug":"并发编程","link":"/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}],"categories":[{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"JVM","slug":"JVM","link":"/categories/JVM/"},{"name":"Java","slug":"Java","link":"/categories/Java/"}]}